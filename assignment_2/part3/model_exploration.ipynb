{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import train\n",
    "import json\n",
    "from dataset import TextDataset\n",
    "from model import TextGenerationModel\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "    def __str__(self):\n",
    "        return json.dumps(self.__dict__, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    with open (os.path.join(path, 'hyperparams.txt'), 'r') as file:\n",
    "        hp = eval(file.read())\n",
    "    hp = hp.__dict__\n",
    "    checkpoint = torch.load(os.path.join(path, 'checkpoint.pth.tar'), map_location='cpu')\n",
    "\n",
    "    dataset = TextDataset(hp['txt_file'], hp['seq_length'], hp['batch_size'], hp['train_steps'])\n",
    "\n",
    "    model = TextGenerationModel(vocabulary_size=dataset.vocab_size,\n",
    "                                lstm_num_hidden=hp['lstm_num_hidden'],\n",
    "                                lstm_num_layers=hp['lstm_num_layers'],\n",
    "                                dropout=1-hp['dropout_keep_prob'],\n",
    "                                temperature=hp['temperature']\n",
    "                               )\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model, dataset, hp\n",
    "\n",
    "def sample_sequence_30(model, dataset, hp, seed_char=None):\n",
    "    if seed_char:\n",
    "        seed_char = dataset._char_to_ix[seed_char]\n",
    "    else:\n",
    "        seed_char = np.random.choice(range(dataset.vocab_size))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # convert char to one-hot encoding\n",
    "        ins = torch.zeros((1, 1, dataset.vocab_size)).to(dtype=torch.float)\n",
    "        ins[0, 0, seed_char] = 1.0\n",
    "\n",
    "        # initialize hidden state and cell\n",
    "        h_t = torch.zeros(hp['lstm_num_layers'], 1, hp['lstm_num_hidden'], dtype=torch.float)\n",
    "        c_t = torch.zeros(hp['lstm_num_layers'], 1, hp['lstm_num_hidden'], dtype=torch.float)\n",
    "\n",
    "        # predict the next character and feed back into model\n",
    "        predictions = [seed_char]\n",
    "        for t in range(hp['seq_length']):\n",
    "            # compute log-odds\n",
    "            outs, (h_t, c_t), _ = model(ins, (h_t, c_t))\n",
    "            # greedily select character according to prediction\n",
    "            pred_char = torch.argmax(outs, dim=1)\n",
    "            # append to generated sequence\n",
    "            predictions.append(pred_char.item())\n",
    "            # feed back into model\n",
    "            ins = torch.zeros((1, 1, dataset.vocab_size)).to(dtype=torch.float)\n",
    "            ins[0, 0, pred_char.item()] = 1.0\n",
    "    print(dataset.convert_to_string(predictions))\n",
    "    \n",
    "def sample_sequence_longer(model, dataset, hp, seed_sentence, sequence_length):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # convert sentence to one-hot representation\n",
    "        tokenized_sentence = [dataset._char_to_ix[char] for char in seed_sentence]\n",
    "        one_hot_dim = (len(tokenized_sentence), 1, dataset.vocab_size)\n",
    "        one_hot_sentence = torch.zeros(*one_hot_dim, dtype=torch.float).scatter_(2, torch.tensor(tokenized_sentence).reshape(-1,1,1), 1.0)\n",
    "        \n",
    "        # extract the last character as the initial input to character-by-character sampling\n",
    "        ins = one_hot_sentence[-1, :,:].view(1,1,dataset.vocab_size)\n",
    "        # exclude the last character from the sentence on the first forward pass\n",
    "        # to get the hidden state that should be passed with the last character.\n",
    "        one_hot_sentence = one_hot_sentence[:-1, :,:]\n",
    "        \n",
    "\n",
    "        # obtain hidden state over seed sentence without last character.\n",
    "        _, (h_t, c_t), _ = model(one_hot_sentence)\n",
    "        \n",
    "        # predict the next character and feed back into model\n",
    "        predictions = tokenized_sentence\n",
    "        for t in range(sequence_length):\n",
    "            # compute log-odds\n",
    "            outs, (h_t, c_t), _ = model(ins, (h_t, c_t))\n",
    "            # greedily select character according to prediction\n",
    "            pred_char = torch.argmax(outs, dim=1)\n",
    "            # append to generated sequence\n",
    "            predictions.append(pred_char.item())\n",
    "            # feed back into model\n",
    "            ins = torch.zeros((1, 1, dataset.vocab_size)).to(dtype=torch.float)\n",
    "            ins[0, 0, pred_char.item()] = 1.0\n",
    "    print(dataset.convert_to_string(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Constitution Text Generation with T=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize dataset with 2882 characters, 57 unique.\n"
     ]
    }
   ],
   "source": [
    "model, dataset, hp = load_model('models/2018-09-29 01-18.us_constitution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitution,\n",
      "nor prohibited by\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_30(model, dataset, hp, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person shall be held to answer for a capital, or otherwise infamous crime,\n",
      "unless on a presentment or indictment of a Grand Jury, except in cases arising\n",
      "in the land or naval forces, or in the Militia, when in actual service\n",
      "in time of War or public danger;\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_longer(model, dataset, hp, 'No person shall be', 242)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Constitution Text Generation with T=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize dataset with 2882 characters, 57 unique.\n"
     ]
    }
   ],
   "source": [
    "model, dataset, hp = load_model('models/2018-09-30 20-03.us_constitution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitution of the United Stat\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_30(model, dataset, hp, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person shall be held to answer for a capital, or otherwise infamous crime,\n",
      "unless on a presentment or indictment of a Grand Jury, except in cases arising\n",
      "in the land or naval forces, or in the Militia, when in actual service\n",
      "in time of War or public danger;\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_longer(model, dataset, hp, 'No person shall be', 242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Constitution Text Generation with T=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize dataset with 2882 characters, 57 unique.\n"
     ]
    }
   ],
   "source": [
    "model, dataset, hp = load_model('models/2018-09-29 02-02.us_constitution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitution, of certain rights\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_30(model, dataset, hp, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person shall be held to answer for a capital, or otherwise infamous crime,\n",
      "unless on a presentment or indictment of a Grand Jury, except in cases arising\n",
      "in the land or naval forces, or in the Militia, when in actual service\n",
      "in time of War or public danger;\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_longer(model, dataset, hp, 'No person shall be', 242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Constitution Text Generation with T=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize dataset with 2882 characters, 57 unique.\n"
     ]
    }
   ],
   "source": [
    "model, dataset, hp = load_model('models/2018-09-29 01-10.us_constitution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congress September 25, 1789\n",
      "Rat\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_30(model, dataset, hp, 'C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person shall be held to answer for a redress of grievances.\n",
      "\n",
      "\n",
      "II\n",
      "\n",
      "A well-regulated militia, being necessary to the security of a free State,\n",
      "the right of the people.\n",
      "\n",
      "X\n",
      "\n",
      "The powers not delegated to the United States Bill of Rights.\n"
     ]
    }
   ],
   "source": [
    "sample_sequence_longer(model, dataset, hp, 'No person shall be', 216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample with exogenous T=0.5,1.0,1.5,2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently not possible because the network is outputting only log-odds, no energies\n",
    "\n",
    "# seed_char = np.random.choice(range(dataset.vocab_size))\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     for T in [0.5,1.0,1.5,2.0]:\n",
    "#         # convert char to one-hot encoding\n",
    "#         ins = torch.zeros((1, 1, dataset.vocab_size)).to(dtype=torch.float)\n",
    "#         ins[0, 0, seed_char] = 1.0\n",
    "\n",
    "#         # initialize hidden state and cell\n",
    "#         h_t = torch.zeros(hp['lstm_num_layers'], 1, hp['lstm_num_hidden']).to(dtype=torch.float)\n",
    "#         c_t = torch.zeros(hp['lstm_num_layers'], 1, hp['lstm_num_hidden']).to(dtype=torch.float)\n",
    "\n",
    "#         # predict the next character and feed back into model\n",
    "#         predictions = []\n",
    "#         for t in range(hp['seq_length']):\n",
    "#             # compute log-odds\n",
    "#             outs, (h_t, c_t) = model(ins, (h_t, c_t))\n",
    "#             # compute annealed distribution\n",
    "#             outs = torch.softmax(outs / T, dim=2)\n",
    "#             # sample character according to prediction\n",
    "#             pred_char = np.random.choice(np.array(range(dataset.vocab_size)), p=outs.numpy().squeeze())\n",
    "#             # append to generated sequence\n",
    "#             predictions.append(pred_char.item())\n",
    "#             # feed back into model\n",
    "#             ins = torch.zeros((1, 1, dataset.vocab_size)).to(dtype=torch.float)\n",
    "#             ins[0, 0, pred_char.item()] = 1.0\n",
    "#         print(T + ':' + ' ' + dataset.convert_to_string(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
